{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 什么是损失函数？\n",
    "通俗解释：损失函数是用来衡量模型预测结果与真实结果之间差距的函数。你可以把它想象成一个“打分器”，告诉模型它的预测有多糟糕。\n",
    "\n",
    "作用：损失函数的值越小，说明模型的预测越接近真实结果；值越大，说明预测越离谱。\n",
    "\n",
    "例子：\n",
    "\n",
    "如果你预测明天会下雨，但实际没下雨，损失函数会告诉你：“你的预测错了，扣分！”\n",
    "\n",
    "如果你预测对了，损失函数会说：“不错，继续保持！”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 什么是训练？\n",
    "通俗解释：训练就是让模型通过不断学习，逐渐减少损失函数的值，从而提高预测的准确性。\n",
    "\n",
    "过程：\n",
    "\n",
    "模型先随机猜测一个结果（比如随机预测一张图片是猫还是狗）。\n",
    "\n",
    "损失函数告诉模型：“你猜错了，扣分！”\n",
    "\n",
    "模型根据损失函数的反馈，调整自己的参数（比如调整卷积核的权重），争取下次猜得更准。\n",
    "\n",
    "重复这个过程很多次，模型就会越来越聪明，预测也越来越准。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 什么是交叉熵损失函数（Cross-Entropy Loss）？\n",
    "通俗解释：交叉熵是用来衡量模型预测的概率分布与真实概率分布之间差距的函数。它常用于分类问题（比如判断一张图片是猫还是狗）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.31326165795326233\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    手动实现交叉熵损失函数\n",
    "    :param y_pred: 模型的预测值（未经过softmax的概率值，形状为 [batch_size, num_classes]）\n",
    "    :param y_true: 真实标签（one-hot编码，形状为 [batch_size, num_classes]）\n",
    "    :return: 交叉熵损失值\n",
    "    \"\"\"\n",
    "    # 对预测值进行softmax，得到概率分布\n",
    "    softmax = torch.exp(y_pred) / torch.sum(torch.exp(y_pred), dim=1, keepdim=True)\n",
    "\n",
    "    # 计算交叉熵损失\n",
    "    loss = -torch.sum(y_true * torch.log(softmax)) / y_true.shape[0]  # 取平均值\n",
    "    return loss\n",
    "\n",
    "# 示例\n",
    "y_pred = torch.tensor([[2.0, 1.0], [1.0, 2.0]])  # 模型的原始输出（未经过softmax）\n",
    "y_true = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # 真实标签（one-hot编码）\n",
    "\n",
    "loss = cross_entropy_loss(y_pred, y_true)\n",
    "print(\"Cross-Entropy Loss:\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orientor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
